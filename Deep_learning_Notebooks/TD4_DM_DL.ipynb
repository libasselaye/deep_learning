{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TD4 : classification à l'aide d'un RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce notebook a été développé dans le cours donné par J. Velcin et J. Cugliari sur le Deep Learning à l'Université de Lyon 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les données en entrée sont toujours les mêmes que pour les TD 2 et 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "with open(os.path.join(\"SMSSpamCollection.txt\")) as f:\n",
    "    lines = [line.strip().split(\"\\t\") for line in f.readlines()]\n",
    "\n",
    "text = [x[1] for x in lines]\n",
    "y = [int(x[0] == \"spam\") for x in lines]\n",
    "\n",
    "vocab_size = 10000\n",
    "max_length = 10\n",
    "\n",
    "encoded_docs = [one_hot(d, vocab_size) for d in text]\n",
    "padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_X, test_X, train_y, test_y = train_test_split(padded_docs, y, \n",
    "                                                    train_size=0.7,\n",
    "                                                    test_size=0.3,\n",
    "                                                    random_state=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette fois, on remplace le MLP par un réseau \"simple\" récurrent comportant 4 cellules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " simple_rnn (SimpleRNN)      (None, 4)                 40020     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 5         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 40,025\n",
      "Trainable params: 40,025\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import SimpleRNN\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "model_rnn = Sequential()\n",
    "model_rnn.add(SimpleRNN(4, return_sequences=False, input_shape=(max_length, vocab_size)))\n",
    "model_rnn.add(Dense(1, activation='sigmoid'))\n",
    "model_rnn.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "# l'optimiseur \"rmsprop\" est considéré comme plus efficace pour les RNN\n",
    "\n",
    "print(model_rnn.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quelques observations sur ce réseau :\n",
    "\n",
    "- L'entrée est donnée sous la forme d'un tenseur (nombre d'observations, nombre de mots dans une séquence, nombre de caractéristiques pour un mot).\n",
    "\n",
    "- Pour chaque observation (texte de n mots), une seule sortie est attendue (return_sequences=False).\n",
    "\n",
    "- Ici, les deux dernières couches comportent respectivement 4 cellules (+1 pour le biais) et 1 cellule pour la sortie finale (classification binaire)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a besoin de transformer l'index de chaque mot dans sa représentation binaire correspondante (on parle de \"one hot encoding\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "train_X_binary = to_categorical(train_X, num_classes=vocab_size)\n",
    "test_X_binary = to_categorical(test_X, num_classes=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_binary.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X_binary.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# il peut être parfois nécessaire de \"ré-arranger\" la forme du tenseur...\n",
    "#train_X_binary = train_X_binary.reshape((train_X.shape[0], 10, 10000))\n",
    "#test_X_binary = test_X_binary.reshape((test_X.shape[0], 10, 10000))\n",
    "\n",
    "import numpy as np\n",
    "train_y = np.array(train_y)\n",
    "test_y = np.array(test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut à présent entraîner le modèle, en faisant attention à bien prendre la version \"binarisée\" des données d'entrée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rnn.fit(train_X_binary, train_y, batch_size=10, epochs=10, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut regarder la prédiction du modèle sur les données de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = model_rnn.evaluate(test_X_binary, test_y, verbose=1)\n",
    "print()\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut bien sûr s'arrêter à l'étape de prédiction afin de mieux contrôler l'utilisation / l'évaluation du modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_y_rnn = model_rnn.predict(test_X_binary, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(pred_y_rnn))\n",
    "print(pred_y_rnn[0:10])\n",
    "print(test_y[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import zero_one_loss\n",
    "\n",
    "pred_y_rnn_binary = [int(x) for x in (pred_y_rnn > 0.5)]\n",
    "e=zero_one_loss(test_y, pred_y_rnn_binary)\n",
    "print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut bien sûr ajouter une couche de plongement de mots (word embedding) au RNN.\n",
    "\n",
    "Il s'agit de ne pas oublier de revenir à une représentation de type \"index\" pour l'entrée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding\n",
    "\n",
    "size_embedding = 20\n",
    "\n",
    "model_rnn_with_embedding = Sequential()\n",
    "# here is the additional layer (note that we add 50*10000 parameters!)\n",
    "model_rnn_with_embedding.add(Embedding(output_dim=size_embedding, input_dim=vocab_size, input_length=max_length))\n",
    "model_rnn_with_embedding.add(SimpleRNN(4, return_sequences=False))\n",
    "model_rnn_with_embedding.add(Dense(1, activation='sigmoid'))\n",
    "model_rnn_with_embedding.compile(optimizer='rmsprop', loss='binary_crossentropy')\n",
    "\n",
    "print(model_rnn_with_embedding.summary())\n",
    "\n",
    "model_rnn_with_embedding.fit(train_X, train_y, batch_size=10, epochs=10, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_y_rnn_with_embedding = model_rnn_with_embedding.predict(test_X, verbose=1)\n",
    "pred_y_rnn_binary_with_embedding = [int(x) for x in (pred_y_rnn_with_embedding > 0.5)]\n",
    "e=zero_one_loss(test_y, pred_y_rnn_binary_with_embedding)\n",
    "print()\n",
    "print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A noter qu'il est toujours possible d'initialiser (bootstrap) la couche d'embedding à l'aide des représentations estimées sur un corpus externe (ex. wikipedia)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
