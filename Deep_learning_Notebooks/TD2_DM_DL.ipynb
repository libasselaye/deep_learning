{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TD2 : classification de données textuelles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce notebook a été développé dans le cours donné par J. Velcin et J. Cugliari sur le Deep Learning à l'Université de Lyon 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On commence par charger en mémoire les données spam diffusée à l'occasion du tutoriel de A. Gramfort et A. Mueller à SciPy 2017\n",
    "https://github.com/amueller/scipy-2017-sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import os\n",
    "\n",
    "with open(os.path.join(\"SMSSpamCollection.txt\")) as f:\n",
    "    lines = [line.strip().split(\"\\t\") for line in f.readlines()]\n",
    "\n",
    "text = [x[1] for x in lines]\n",
    "y = [int(x[0] == \"spam\") for x in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...', 'Ok lar... Joking wif u oni...', \"Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\", 'U dun say so early hor... U c already then say...', \"Nah I don't think he goes to usf, he lives around here though\"]\n",
      "[0, 0, 1, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(text[0:5])\n",
    "print(y[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La bibliothèque scikit-learn fournit des commandes très utiles pour vectoriser le texte, cf. tutoriel SciPy 2017 et cours de text mining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorize_spamdata = TfidfVectorizer()\n",
    "vectorize_spamdata.fit(text)\n",
    "data = vectorize_spamdata.transform(text).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['00', '000', '000pes', ..., 'zoom', 'zouk', 'zyada'], dtype=object)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#il va decouper le document en mots d'ou les 5574 doc contiennent 8716 mots différents\n",
    "vectorize_spamdata.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5574, 8716)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim = data.shape[1]\n",
    "print(data.shape)\n",
    "data[10:20, 4:8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On sépare le jeu de données en ensemble d'entraînement et de test en conservant un équilibre dans les classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_X, test_X, train_y, test_y = train_test_split(data, y, \n",
    "                                                    train_size=0.7,\n",
    "                                                    test_size=0.3,\n",
    "                                                    random_state=123,\n",
    "                                                   stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On construit un simple MLP avec une couche cachée, cf. TD 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "def mlp():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(8, input_dim=dim, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 8)                 69736     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 69,745\n",
      "Trainable params: 69,745\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "simple_mlp = mlp()\n",
    "simple_mlp.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a besoin de convertir la liste en tableau numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y = np.array(test_y)\n",
    "train_y = np.array(train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On lance l'apprentissage sur 10 epochs avec des batch de 10 textes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "391/391 [==============================] - 3s 4ms/step - loss: 0.4086 - accuracy: 0.8688\n",
      "Epoch 2/10\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.1679 - accuracy: 0.9431\n",
      "Epoch 3/10\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 0.0849 - accuracy: 0.9787\n",
      "Epoch 4/10\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.0487 - accuracy: 0.9903\n",
      "Epoch 5/10\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.0305 - accuracy: 0.9941\n",
      "Epoch 6/10\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 0.0206 - accuracy: 0.9956\n",
      "Epoch 7/10\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 0.0147 - accuracy: 0.9982\n",
      "Epoch 8/10\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.0109 - accuracy: 0.9987\n",
      "Epoch 9/10\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.0081 - accuracy: 0.9990\n",
      "Epoch 10/10\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.0062 - accuracy: 0.9992\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x23cc78c85b0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_mlp.fit(train_X, train_y, epochs=10, batch_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voyons les résultats en généralisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 1s 3ms/step - loss: 0.0513 - accuracy: 0.9863\n",
      "test score:  0.051328688859939575\n",
      "test accuracy:  0.9862522482872009\n"
     ]
    }
   ],
   "source": [
    "score = simple_mlp.evaluate(test_X, test_y)\n",
    "print(\"test score: \", score[0])\n",
    "print(\"test accuracy: \", score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour information, la régression logistique atteint ~96% de réussite sur le même jeu de données."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
